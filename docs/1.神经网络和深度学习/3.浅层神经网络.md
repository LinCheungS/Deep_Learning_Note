# 浅层神经网络

## 1.神经网络表示(neural network represent)

**输入层**(the input layer): 竖向堆叠起来的输入特征  
**隐藏层**(hidden layer): 在训练集中,这些中间节点的真正数值是无法看到的  
**输出层**(the output layer): 负责输出预测值  

![single_hidden_layer_neural_network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Neural_Networks_and_Deep_Learning/single_hidden_layer_neural_network.png)  

如图是一个 **双层神经网络**，也称作 **单隐层神经网络**(single hidden layer neural network).当我们计算网络的层数时，通常不考虑输入层，因此图中隐藏层是第一层，输出层是第二层。  

约定俗成的 **符号表示**是：$a_{[3]}^{[1][2]}$, 符号[1]表示的是第几**层**(layer), 符号[2]表示的是第几个**训练样本**(trainning example), 符号[3]表示的是第几个**神经元**(neural)

## 2.神经网络计算(neural network compute)

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609115447.png)  
![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609113931.png)  

* 正向传播  

    - 向量化前  
        * $for\ i\ =\ 1\ to\ m:$  
        * $z^{[1](i)}=W^{[1](i)}x^{(i)}+b^{[1](i)}$  
        * $a^{[1](i)}=\sigma(z^{[1](i)})$  
        * $z^{[2](i)}=W^{[2](i)}a^{[1](i)}+b^{[2](i)}$  
        * $a^{[2](i)}=\sigma(z^{[2](i)})$  
    - 向量化后  
        * $z _1^{[1]} = (W _1^{[1]})^TX+b _1^{[1]}$  
        * $a _1^{[1]} = \sigma(z _1^{[1]})$  
        * $z^{[2]} = (W^{[2]})^Ta^{[1]}+b^{[2]}$  
        * $\hat{y} = a^{[2]} = \sigma(z^{[2]})$  

    - 矩阵的shape关系: 
        * ${(W^{[1]})}^T$的 shape 为`(4,3)`，4 是**本层神经元**的个数，3 是**上层神经元**的个数  
        * $b^{[1]}$的 shape 为`(4,1)`，和**本层的神经元**个数相同。  
        * ${(W^{[2]})}^T$的 shape 为`(1,4)`，1 是**本层神经元**的个数，4 是**上层神经元**的个数  
        * $b^{[2]}$的 shape 为`(1,1)`，和**本层的神经元**个数相同。  

* 反向传播  
    - 由最深层至最浅层依次求导
        * $d z^{[2]}=a^{[2]}-y$  
        * $d W^{[2]}=d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial W^{|2|}}=d z^{[2]} a^{[1] T}$  
        * $d b^{[2]}=d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial b^{(2)}}=d z^{[2]} \cdot 1=d z^{[2]}$  
        * $d z^{[1]}=d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2] T} d z^{[2]} * g^{[1]^{\prime}}\left(z^{[1]}\right)$  
        * $d W^{[1]}=d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=d z^{[1]} x^{T}$  
        * $d b^{[1]}=d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=d z^{[1]} \cdot 1=d z^{[1]}$   

    + $d Z^{[1]}$ 求导[Coursera上的证明](https://www.coursera.org/learn/neural-networks-deep-learning/discussions/weeks/3/threads/uNNlIM3REeeDkBL7Tv06uA)  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609122542.png)  
  
* 更新参数
    + W1 -= learning_rate * dW1  
    + b1 -= learning_rate * db1  
    + W2 -= learning_rate * dW2  
    + b2 -= learning_rate * db2  


## 3.激活函数(Activation functions)

- 为什么需要激活函数?  
    如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。  

- 常用的激活函数  
![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609125853.png)  
![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609125906.png)  
    - **sigmoid函数** 因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数
    - **tanh 函数**(the hyperbolic tangent function，双曲正切函数(效果几乎总比 sigmoid 函数好（除开**二元分类的输出层**，因为我们希望输出的结果介于 0 到 1 之间），因为函数输出介于 -1 和 1 之间，激活函数的平均值就更接近 0，有类似数据中心化的效果。然而，tanh 函数存在和 sigmoid 函数一样的缺点：当 z 趋紧无穷大（或无穷小），导数的梯度（即函数的斜率）就趋紧于 0，这使得梯度算法的速度大大减缓。
    - **ReLU 函数**(the rectified linear unit，修正线性单元)当 z > 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于 sigmoid 和 tanh。然而当 z < 0 时，梯度一直为 0，但是实际的运用中，该缺陷的影响不是很大。
    - **Leaky ReLU**（带泄漏的 ReLU）：Leaky ReLU 保证在 z < 0 的时候，梯度仍然不为 0。理论上来说，Leaky ReLU 有 ReLU 的所有优点，但在实际操作中没有证明总是好于 ReLU，因此不常用。

- 激活函数求导
    + sigmoid函数的导数:  
    $g(z)=\frac{1}{1+e^{(-z)}}$  
    $g^{\prime}(z)=\frac{d}{d z} g(z)=g(z)(1-g(z))=a(1-a)$  
    + 对于tanh函数的导数：  
    $g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}}$  
    $g^{\prime}(z)=\frac{d}{d z} g(z)=1-(g(z))^{2}=1-a^{2}$  
    + 对于ReLU函数的导数：  
    $g(z)=\max (0, z)$  
    $g^{\prime}(z)=\{0, \quad z<01, z \geq 0$  
    + 对于Leaky ReLU函数：  
    $g(z)=\max (0.01 z, z)$  
    $g^{\prime}(z)=\{0.01, \quad z<01, z \geq 0$  

## 4.初始化权重(Initialization)

如果在初始时将两个隐藏神经元的参数设置为**相同的大小**，那么两个隐藏神经元对输出单元的**影响也是相同的**，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是**对称的**。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。

在初始化的时候，W 参数要进行随机初始化，不可以设置为 0。而 b 因为不存在对称性的问题，可以设置为 0。 以 2 个输入，2 个隐藏神经元为例：

```py
W = np.random.rand(2,2)* 0.01
b = np.zeros((2,1))
```

这里将 W 的值乘以 0.01（或者其他的常数值）的原因是为了使得权重 W 初始化为较小的值，这是因为使用 sigmoid 函数或者 tanh 函数作为激活函数时，W 比较小，则 Z=WX+b 所得的值趋近于 0，梯度较大，能够提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。

ReLU 和 Leaky ReLU 作为激活函数时不存在这种问题，因为在大于 0 的时候，梯度均为 1。

## **5.代码实现**
```python
"""根据数据集定义神经网络大小"""
def layer_sizes(X, Y):

    n_x = X.shape[0] # 输入层大小
    n_h = 4 # 神经元个数
    n_y = Y.shape[0] # 输出层大小

    return (n_x, n_h, n_y)
```
```python
"""初始化参数"""
def initialize_parameters(n_x, n_h, n_y):
    
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros(shape=(n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros(shape=(n_y, 1))
    
    parameters = {"W1": W1,"b1": b1,"W2": W2,"b2": b2}
    
    return parameters
```
```python
"""正向传播"""
def forward_propagation(X, parameters):
    
    # 获得初始化过得参数
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    # 正向传播
    Z1 = W1 @ X + b1
    A1 = np.tanh(Z1)
    Z2 = W2 @ A1 + b2
    A2 = sigmoid(Z2)
    
    cache = {"Z1": Z1,"A1": A1,"Z2": Z2,"A2": A2}
    
    return A2, cache
```
```python
"""计算代价函数"""
def compute_cost(A2, Y, parameters):
    # A2= 最后一层的输出
    
    m = Y.shape[1] # 数据集的样本数
    cost = -1/m * np.sum(Y * np.log(A2) + (1-Y) * np.log(1-A2))
    cost = float(np.squeeze(cost))# 确保输出的维度
    
    return cost
```
```python
"""计算反向传播"""
def backward_propagation(parameters, cache, X, Y):
    # cache = 正向传播中的中间值,比如A1,A2,Z1,Z2
    
    m = X.shape[1]
    W1 = parameters['W1']
    W2 = parameters['W2']
    A1 = cache['A1']
    A2 = cache['A2']
    
    # 计算每一层神经网络上,对于参数w,b的偏导 
    dZ2 = A2 - Y
    dW2 = 1/m * dZ2 @ A1.T
    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = W2.T @ dZ2 * (1 - A1**2) 
    dW1 = 1/m * dZ1 @ X.T
    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)
    
    grads = {"dW1": dW1,"db1": db1,"dW2": dW2,"db2": db2}
    
    return grads
```
```python
"""更新参数"""
def update_parameters(parameters, grads, learning_rate = 1.2):
    # 参数 w,b
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    # 导数 w,b
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']
    # 更新参数
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2

    parameters = {"W1": W1,"b1": b1,"W2": W2,"b2": b2}
    
    return parameters
```
```python
# 完整的流程
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    # n_h= 隐藏层的神经元数,只有一个隐藏层

    # 1.得到每一层的神经元数
    n_x = layer_sizes(X, Y)[0] # n_x第一层的神经元数
    n_y = layer_sizes(X, Y)[2] # n_y最后一层神经元数
    # 2.初始化参数
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    # 3.总共运行num_iterations次迭代
    for i in range(0, num_iterations):
        # 正向传播, 得到最后一层的A2, 和每一层的中间值 Z1,Z2,A1,A2
        A2, cache = forward_propagation(X, parameters)
        # 根据A2计算代价函数
        cost = compute_cost(A2, Y, parameters)
        # 反向传播得到每一层梯度
        grads = backward_propagation(parameters, cache, X, Y)
        # 根据梯度更新参数
        parameters = update_parameters(parameters, grads)
        # 每1000次进行输出打印
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters
```
```python
"""预测"""
def predict(parameters, X):
    # 获得训练好的参数,进行预测
    A2, cache = forward_propagation(X, parameters)
    predictions =( A2 > 0.5 ) * 1

    return predictions
```
```python
# 调用
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)
```
