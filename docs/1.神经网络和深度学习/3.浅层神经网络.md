# 浅层神经网络

## 神经网络表示(neural network represent)

**输入层**(the input layer): 竖向堆叠起来的输入特征  
**隐藏层**(hidden layer): 在训练集中,这些中间节点的真正数值是无法看到的  
**输出层**(the output layer): 负责输出预测值  

![single_hidden_layer_neural_network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Neural_Networks_and_Deep_Learning/single_hidden_layer_neural_network.png)  

如图是一个 **双层神经网络**，也称作 **单隐层神经网络**(single hidden layer neural network).当我们计算网络的层数时，通常不考虑输入层，因此图中隐藏层是第一层，输出层是第二层。  

约定俗成的 **符号表示**是：$a_{[3]}^{[1][2]}$, 符号[1]表示的是第几**层**(layer), 符号[2]表示的是第几个**训练样本**(trainning example), 符号[3]表示的是第几个**神经元**(neural)

## 神经网络计算(neural network compute)

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609115447.png)  
![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609113931.png)  

* 正向传播  

    - 向量化前  
        * $for\ i\ =\ 1\ to\ m:$  
        * $z^{[1](i)}=W^{[1](i)}x^{(i)}+b^{[1](i)}$  
        * $a^{[1](i)}=\sigma(z^{[1](i)})$  
        * $z^{[2](i)}=W^{[2](i)}a^{[1](i)}+b^{[2](i)}$  
        * $a^{[2](i)}=\sigma(z^{[2](i)})$  
    - 向量化后  
        * $z _1^{[1]} = (W _1^{[1]})^TX+b _1^{[1]}$  
        * $a _1^{[1]} = \sigma(z _1^{[1]})$  
        * $z^{[2]} = (W^{[2]})^Ta^{[1]}+b^{[2]}$  
        * $\hat{y} = a^{[2]} = \sigma(z^{[2]})$  

    - 矩阵的shape关系: 
        * ${(W^{[1]})}^T$的 shape 为`(4,3)`，4 是**本层神经元**的个数，3 是**上层神经元**的个数  
        * $b^{[1]}$的 shape 为`(4,1)`，和**本层的神经元**个数相同。  
        * ${(W^{[2]})}^T$的 shape 为`(1,4)`，1 是**本层神经元**的个数，4 是**上层神经元**的个数  
        * $b^{[2]}$的 shape 为`(1,1)`，和**本层的神经元**个数相同。  

* 反向传播  
    - 由最深层至最浅层依次求导
        * $d z^{[2]}=a^{[2]}-y$  
        * $d W^{[2]}=d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial W^{|2|}}=d z^{[2]} a^{[1] T}$  
        * $d b^{[2]}=d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial b^{(2)}}=d z^{[2]} \cdot 1=d z^{[2]}$  
        * $d z^{[1]}=d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2] T} d z^{[2]} * g^{[1]^{\prime}}\left(z^{[1]}\right)$  
        * $d W^{[1]}=d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=d z^{[1]} x^{T}$  
        * $d b^{[1]}=d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=d z^{[1]} \cdot 1=d z^{[1]}$   

    + $d Z^{[1]}$ 求导[Coursera上的证明](https://www.coursera.org/learn/neural-networks-deep-learning/discussions/weeks/3/threads/uNNlIM3REeeDkBL7Tv06uA)  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609122542.png)  
  
* 更新参数
    + W1 -= learning_rate * dW1  
    + b1 -= learning_rate * db1  
    + W2 -= learning_rate * dW2  
    + b2 -= learning_rate * db2  


## 激活函数(Activation functions)

- 为什么需要激活函数?  
    如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。  

- 常用的激活函数  
![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609125853.png)  
![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200609125906.png)  
    - **sigmoid函数** 因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数
    - **tanh 函数**(the hyperbolic tangent function，双曲正切函数(效果几乎总比 sigmoid 函数好（除开**二元分类的输出层**，因为我们希望输出的结果介于 0 到 1 之间），因为函数输出介于 -1 和 1 之间，激活函数的平均值就更接近 0，有类似数据中心化的效果。然而，tanh 函数存在和 sigmoid 函数一样的缺点：当 z 趋紧无穷大（或无穷小），导数的梯度（即函数的斜率）就趋紧于 0，这使得梯度算法的速度大大减缓。
    - **ReLU 函数**(the rectified linear unit，修正线性单元)当 z > 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于 sigmoid 和 tanh。然而当 z < 0 时，梯度一直为 0，但是实际的运用中，该缺陷的影响不是很大。
    - **Leaky ReLU**（带泄漏的 ReLU）：Leaky ReLU 保证在 z < 0 的时候，梯度仍然不为 0。理论上来说，Leaky ReLU 有 ReLU 的所有优点，但在实际操作中没有证明总是好于 ReLU，因此不常用。

- 激活函数求导
    + sigmoid函数的导数:  
    $g(z)=\frac{1}{1+e^{(-z)}}$  
    $g^{\prime}(z)=\frac{d}{d z} g(z)=g(z)(1-g(z))=a(1-a)$  
    + 对于tanh函数的导数：  
    $g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}}$  
    $g^{\prime}(z)=\frac{d}{d z} g(z)=1-(g(z))^{2}=1-a^{2}$  
    + 对于ReLU函数的导数：  
    $g(z)=\max (0, z)$  
    $g^{\prime}(z)=\{0, \quad z<01, z \geq 0$  
    + 对于Leaky ReLU函数：  
    $g(z)=\max (0.01 z, z)$  
    $g^{\prime}(z)=\{0.01, \quad z<01, z \geq 0$  

## 初始化权重(Initialization)

如果在初始时将两个隐藏神经元的参数设置为**相同的大小**，那么两个隐藏神经元对输出单元的**影响也是相同的**，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是**对称的**。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。

在初始化的时候，W 参数要进行随机初始化，不可以设置为 0。而 b 因为不存在对称性的问题，可以设置为 0。 以 2 个输入，2 个隐藏神经元为例：

```py
W = np.random.rand(2,2)* 0.01
b = np.zeros((2,1))
```

这里将 W 的值乘以 0.01（或者其他的常数值）的原因是为了使得权重 W 初始化为较小的值，这是因为使用 sigmoid 函数或者 tanh 函数作为激活函数时，W 比较小，则 Z=WX+b 所得的值趋近于 0，梯度较大，能够提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。

ReLU 和 Leaky ReLU 作为激活函数时不存在这种问题，因为在大于 0 的时候，梯度均为 1。





