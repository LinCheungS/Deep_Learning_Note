# 深层神经网络

## 1.深层神经网络概述  

深层神经网络包含更多的隐藏层神经网络。命名规则上，一般只包含**隐藏层**和**输出层**。例如下图中分别是一层,两层,三层,六层神经网络, 其中有零个,一个,两个,五个隐藏层.

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200611234051.png)  

例如人脸识别, 第一层所能从原始图片中提取出人脸的轮廓与边缘，即边缘检测。第二层所能将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。第三层就能将这些局部特征组合起来，融合成人脸的模样。神经网络从左到右，神经元提取的特征从简单到复杂.**特征复杂度与神经网络层数成正相关**。特征越来越复杂，功能也越来越强大。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200611234902.png)

## 2.深层神经网络推导

对于第l层来说，正向传播过程中：  

**输入**：$a^{[l−1]}$

**输出**：$a^{[l]}$，cache($z^{[l]}$)

**公式**：

$$Z^{[l]}=W^{[l]}\cdot a^{[l-1]}+b^{[l]}$$

$$a^{[l]}=g^{[l]}(Z^{[l]})$$

对于第l层来说, 反向传播过程中：

**输入**：$da^{[l]}$

**输出**：$da^{[l-1]}$，$dW^{[l]}$，$db^{[l]}$

**公式**：

$$dZ^{[l]}=da^{[l]}*g^{[l]}{'}(Z^{[l]})$$

$$dW^{[l]}=dZ^{[l]}\cdot a^{[l-1]}$$

$$db^{[l]}=dZ^{[l]}$$

$$da^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}$$  

对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示:  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200612000455.png)  

首先是正向传播过程, 令层数为第 $l$ 层, 输入是 $a^{[l-1]},$ 输出是 $a^{[l]},$ 缓存变量是 $z^{[l]}$ 。其表达式如下:
$$z^{[l]}=W^{[l]} a^{[l-1]}+b^{[l]}$$
$$a^{[l]}=g^{[l]}\left(z^{[l]}\right)$$

m个训练样本, 向量化形式为
$$Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]}$$
$$A^{[l]}=g^{[l]}\left(Z^{[l]}\right)$$

然后是反向传播过程, 输入是 $d a^{[l]},$ 输出是 $d a^{[l-1]}, d w^{[l]}, d b^{[l]}$ 。其表达式如下：
$$d z^{[l]}=d a^{[l]} * g^{[l]}{'}\left(z^{[l]}\right)$$
$$d W^{[l]}=d z^{[l]} \cdot a^{[l-1]}$$
$$d b^{[l]}=d z^{[l]}$$
$$d a^{[l-1]}=W^{[l] T} \cdot d z^{[l]}$$

由上述第四个表达式可得 $d a^{[l]}=W^{[l+1] T} \cdot d z^{[l+1]},$ 将 $d a^{[l]}$ 代入第一个表达式中可以得到
$$d z^{[l]}=W^{[l+1] T} \cdot d z^{[l+1]} * g^{[l]}{'}\left(z^{[l]}\right)$$

该式非常重要，反映了 $d z^{[l+1]}$ 和 $d z^{[l]}$ 的递推关系  
m个训练样本，向量化形式为：
$$d Z^{[l]}=d A^{[l]} * g^{[l]}\left(Z^{[l]}\right) \\
d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T} \\
d b^{[l]}=\frac{1}{m} n p . s u m\left(d Z^{[l]}, a x i s=1, k e e p d i m=T r u e\right) \\
d A^{[l-1]}=W^{[l] T} \cdot d Z^{[l]} \\
d Z^{[l]}=W^{[l+1] T} \cdot d Z^{[l+1]} * g^{[l]^{\prime}}\left(Z^{[l]}\right)$$  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200612012358.png)

## 3.神经网络中的维度(Dimensions)

$w$的维度是(这一层的维数，前一层的维数): ${{w}^{[l]}}$: (${{n}^{[l]}}$,${{n}^{[l-1]}}$)
$b$的维度是(这一层的维数，1):${{b}^{[l]}}$ : (${{n}^{[l]}},1)$；

Z和a的维度是(这一层的维数，1)${{z}^{[l]}}$,${{a}^{[l]}}$: $({{n}^{[l]}},1)$;

${{dw}^{[l]}}$和${{w}^{[l]}}$维度相同，${{db}^{[l]}}$和${{b}^{[l]}}$维度相同，且$w$和$b$向量化维度不变，但$z$,$a$以及$x$的维度会向量化后发生变化。

向量化后：

${Z}^{[l]}$可以看成由每一个单独的${Z}^{[l]}$叠加而得到，${Z}^{[l]}=({{z}^{[l][1]}}，{{z}^{[l][2]}}，{{z}^{[l][3]}}，…，{{z}^{[l][m]}})$，

$m$为训练集大小，所以${Z}^{[l]}$的维度不再是$({{n}^{[l]}},1)$，而是$({{n}^{[l]}},m)$。

${A}^{[l]}$：$({n}^{[l]},m)$，${A}^{[0]} = X =({n}^{[l]},m)$  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200612012157.png)  

## 4.神经网络中的超参数(Hyperparameters) 

**参数**即是我们在过程中想要模型学习到的信息（**模型自己能计算出来的**），例如 $W^{[l]}$，$b^{[l]}$。而**超参数（hyper parameters）**即为控制参数的输出值的一些网络信息（**需要人经验判断**）。超参数的改变会导致最终得到的参数 $W^{[l]}$，$b^{[l]}$ 的改变。

典型的超参数有：

* 学习速率：α
* 迭代次数：N
* 隐藏层的层数：L
* 每一层的神经元个数：$n^{[1]}$，$n^{[2]}$，...
* 激活函数 g(z) 的选择

当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。

## **5.代码实现**
```python
"""初始化参数"""
def initialize_parameters_deep(layer_dims):
    # layers_dims = [12288, 20, 7, 5, 1] #  4-layer model
   
    parameters = {}
    L = len(layer_dims) # 神经网络的层数
    
    # 从1层到L-1层, 只包含隐藏层,不包含输出层
    for l in range(1, L):
        # w = (这一层神经元, 上一层神经元)
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01
        # b = (这一层神经元, 1)
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))

    return parameters
```
```python
def sigmoid(Z):
    A = 1/(1+np.exp(-Z))
    cache = Z
    return A, cache

def relu(Z):
    A = np.maximum(0,Z)
    assert(A.shape == Z.shape)
    cache = Z 
    return A, cache

"""正向传播-线性回归"""
def linear_forward(A, W, b):

    Z = W @ A + b
    cache = (A, W, b)
    
    return Z, cache

"""正向传播-激活函数"""
def linear_activation_forward(A_prev, W, b, activation):
    
    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev, W, b) # cache = (A, W, b)
        A, activation_cache = sigmoid(Z) # cache = Z
    
    elif activation == "relu":
        Z, linear_cache = linear_forward(A_prev, W, b) # cache = (A, W, b)
        A, activation_cache = relu(Z) # cache = Z
    
    cache = (linear_cache, activation_cache) # cache= ((A, W, b),(Z))
    # 返回最终的A,和cache
    return A, cache

"""正向传播"""
def L_model_forward(X, parameters):
    
    caches = []
    A = X # 第一层A=X
    L = len(parameters) // 2 # 字典中含有"w"和"b",所以要除以2
    
    # l的范围是1到L-1,包含所有隐藏层使用relu
    for l in range(1, L):
        A_prev = A 
        # 执行 Z = wx+b, 并且将Z传给激活函数sigmod或者relu
        # 返回:最终的A, 和cache= ((A, W, b),(Z))
        A, cache = linear_activation_forward(A_prev, parameters['W{:d}'.format(l)], parameters['b{:d}'.format(l)], activation='relu')
        caches.append(cache)
    
    # 最后一层使用sigmod,执行 Z = wx+b, 并且将Z传给激活函数sigmod或者relu,返回:最终的A, 和cache= ((A, W, b),(Z))
    AL, cache = linear_activation_forward(A, parameters['W%d' % L], parameters['b%d' % L], activation='sigmoid')
    caches.append(cache)
    
    # 返回最终的A和caches
    return AL, caches
```
```python
"""计算代价函数"""
def compute_cost(AL, Y):

    m = Y.shape[1]
    cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL))
    cost = np.squeeze(cost)
    
    return cost
```
```python
def relu_backward(dA, cache):
    Z = cache
    dZ = np.array(dA, copy=True) # just converting dz to a correct object.
    dZ[Z <= 0] = 0 # When z <= 0, you should set dz to 0 as well. 
    assert (dZ.shape == Z.shape)
    return dZ

def sigmoid_backward(dA, cache):
    Z = cache
    a = 1/(1+np.exp(-Z))
    dZ = dA * a * (1-a)
    assert (dZ.shape == Z.shape)
    return dZ

"""反向传播-线性回归"""
def linear_backward(dZ, cache):
    # cache -- 本层正向传播的值(A_prev, W, b)

    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = 1 / m * dZ @ A_prev.T
    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = W.T @ dZ
    
    return dA_prev, dW, db

"""反向传播-激活函数"""
def linear_activation_backward(dA, cache, activation):
    # cache = (正向传播线性回归值, 正向传播激活函数值) ((A, W, b),(Z))

    linear_cache, activation_cache = cache
    
    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
        
    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    
    return dA_prev, dW, db

"""反向传播"""
def L_model_backward(AL, Y, caches):
    # AL = 最后一层结果
    
    grads = {}
    L = len(caches) # layer层数
    m = AL.shape[1] # 数据集样本数
    Y = Y.reshape(AL.shape) # 保证维度统一
    
    # 进行L层, 最后一层
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    
    # 进行到(L-1), linear_activation_backward包含激活函数求导, 线性回归求导
    current_cache = caches[L-1]
    grads["dA" + str(L-1)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')
    
    # 进行到(L-2)层, for循环(L-2)--0
    for l in reversed(range(L-1)):
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l + 1)], current_cache, 'relu')
        grads["dA" + str(l)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp

    return grads
```
```python
"""更新参数"""
def update_parameters(parameters, grads, learning_rate):
    
    L = len(parameters) // 2 # number of layers in the neural network

    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)]

    return parameters
```
```python
"""调用函数"""
def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009

    np.random.seed(1)
    costs = [] # keep track of cost
    
    # 1.初始化参数
    parameters = initialize_parameters_deep(layers_dims)

    for i in range(0, num_iterations):
        # 2. 正向传播
        AL, caches = L_model_forward(X, parameters)
        # 3. 计算代价函数
        cost = compute_cost(AL, Y)
        # 4. 反向传播
        grads =  L_model_backward(AL, Y, caches)
        # 5. 更新参数
        parameters = update_parameters(parameters, grads, learning_rate)
        # 打印细节
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)

    # plot the cost
    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
```
```python
layers_dims = [12288, 20, 7, 5, 1] #  4-layer model
parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)  
# Cost after iteration 0: 0.771749
# Cost after iteration 100: 0.672053
# Cost after iteration 200: 0.648263
```