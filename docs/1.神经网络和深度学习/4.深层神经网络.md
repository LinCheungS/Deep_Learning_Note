# 深层神经网络

## 深层神经网络概述  

深层神经网络包含更多的隐藏层神经网络。命名规则上，一般只包含**隐藏层**和**输出层**。例如下图中分别是一层,两层,三层,六层神经网络, 其中有零个,一个,两个,五个隐藏层.

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200611234051.png)  

例如人脸识别, 第一层所能从原始图片中提取出人脸的轮廓与边缘，即边缘检测。第二层所能将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。第三层就能将这些局部特征组合起来，融合成人脸的模样。神经网络从左到右，神经元提取的特征从简单到复杂.**特征复杂度与神经网络层数成正相关**。特征越来越复杂，功能也越来越强大。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200611234902.png)

## 深层神经网络推导

对于第l层来说，正向传播过程中：  

**输入**：$a^{[l−1]}$

**输出**：$a^{[l]}$，cache($z^{[l]}$)

**公式**：

$$Z^{[l]}=W^{[l]}\cdot a^{[l-1]}+b^{[l]}$$

$$a^{[l]}=g^{[l]}(Z^{[l]})$$

对于第l层来说, 反向传播过程中：

**输入**：$da^{[l]}$

**输出**：$da^{[l-1]}$，$dW^{[l]}$，$db^{[l]}$

**公式**：

$$dZ^{[l]}=da^{[l]}*g^{[l]}{'}(Z^{[l]})$$

$$dW^{[l]}=dZ^{[l]}\cdot a^{[l-1]}$$

$$db^{[l]}=dZ^{[l]}$$

$$da^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}$$  

对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示:  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200612000455.png)  

首先是正向传播过程, 令层数为第 $l$ 层, 输入是 $a^{[l-1]},$ 输出是 $a^{[l]},$ 缓存变量是 $z^{[l]}$ 。其表达式如下:
$$z^{[l]}=W^{[l]} a^{[l-1]}+b^{[l]}$$
$$a^{[l]}=g^{[l]}\left(z^{[l]}\right)$$

m个训练样本, 向量化形式为
$$Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]}$$
$$A^{[l]}=g^{[l]}\left(Z^{[l]}\right)$$

然后是反向传播过程, 输入是 $d a^{[l]},$ 输出是 $d a^{[l-1]}, d w^{[l]}, d b^{[l]}$ 。其表达式如下：
$$d z^{[l]}=d a^{[l]} * g^{[l]}\left(z^{[l]}\right)$$
$$d W^{[l]}=d z^{[l]} \cdot a^{[l-1]}$$
$$d b^{[l]}=d z^{[l]}$$
$$d a^{[l-1]}=W^{[l] T} \cdot d z^{[l]}$$

由上述第四个表达式可得 $d a^{[l]}=W^{[l+1] T} \cdot d z^{[l+1]},$ 将 $d a^{[l]}$ 代入第一个表达式中可以得到
$$d z^{[l]}=W^{[l+1] T} \cdot d z^{[l+1]} * g^{[l]}\left(z^{[l]}\right)$$

该式非常重要，反映了 $d z^{[l+1]}$ 和 $d z^{[l]}$ 的递推关系  
m个训练样本，向量化形式为：
$$d Z^{[l]}=d A^{[l]} * g^{[l]}\left(Z^{[l]}\right) \\
d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T} \\
d b^{[l]}=\frac{1}{m} n p . s u m\left(d Z^{[l]}, a x i s=1, k e e p d i m=T r u e\right) \\
d A^{[l-1]}=W^{[l] T} \cdot d Z^{[l]} \\
d Z^{[l]}=W^{[l+1] T} \cdot d Z^{[l+1]} * g^{[l]^{\prime}}\left(Z^{[l]}\right)$$  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200612012358.png)

## 神经网络中的维度(Dimensions)

$w$的维度是(这一层的维数，前一层的维数): ${{w}^{[l]}}$: (${{n}^{[l]}}$,${{n}^{[l-1]}}$)
$b$的维度是(这一层的维数，1):${{b}^{[l]}}$ : (${{n}^{[l]}},1)$；

Z和a的维度是(这一层的维数，1)${{z}^{[l]}}$,${{a}^{[l]}}$: $({{n}^{[l]}},1)$;

${{dw}^{[l]}}$和${{w}^{[l]}}$维度相同，${{db}^{[l]}}$和${{b}^{[l]}}$维度相同，且$w$和$b$向量化维度不变，但$z$,$a$以及$x$的维度会向量化后发生变化。

向量化后：

${Z}^{[l]}$可以看成由每一个单独的${Z}^{[l]}$叠加而得到，${Z}^{[l]}=({{z}^{[l][1]}}，{{z}^{[l][2]}}，{{z}^{[l][3]}}，…，{{z}^{[l][m]}})$，

$m$为训练集大小，所以${Z}^{[l]}$的维度不再是$({{n}^{[l]}},1)$，而是$({{n}^{[l]}},m)$。

${A}^{[l]}$：$({n}^{[l]},m)$，${A}^{[0]} = X =({n}^{[l]},m)$  

![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020-2/20200612012157.png)  

## 神经网络中的超参数(Hyperparameters) 

**参数**即是我们在过程中想要模型学习到的信息（**模型自己能计算出来的**），例如 $W^{[l]}$，$b^{[l]}$。而**超参数（hyper parameters）**即为控制参数的输出值的一些网络信息（**需要人经验判断**）。超参数的改变会导致最终得到的参数 $W^{[l]}$，$b^{[l]}$ 的改变。

典型的超参数有：

* 学习速率：α
* 迭代次数：N
* 隐藏层的层数：L
* 每一层的神经元个数：$n^{[1]}$，$n^{[2]}$，...
* 激活函数 g(z) 的选择

当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。